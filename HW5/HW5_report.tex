\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}

\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\begin{document}

% ========== Edit your name here
\author{Aobo Yang (ay6gv)}
\title{CS6316: HW5}
\maketitle

\medskip

% ========== Begin answering questions here
\begin{enumerate}

\item
Naive Bayes Classifier for Movie Review Text Classification

1.1 Preprocessing

\medskip

Both preprocessing choices are implemented. For the second, after stemming and removing stop words, words appear more than $3$ times in the training data are kept as the vocabulary. The full length of this vocabulary is $10509$. Overall, the second choice performs better in both following classifier. It makes sense since it has more features and the comprehensive stemming also makes the counts more accurate.

\medskip

1.3 Multinomial Naive Bayes Classifier (MNBC) Training Step

\medskip

The resulting values of thetaPos and thetaNeg are show below. Please note that the "UNK" token is removed as requested so the parameter size is $100$.

\begin{verbatim}
thetaPos = [
 0.00215947 0.00287929 0.03050941 0.00166113 0.00088594 0.00548173
 0.00509413 0.00703212 0.00470653 0.00431894 0.00387597 0.00409745
 0.00354374 0.00393134 0.00404208 0.00526024 0.00409745 0.00526024
 0.00387597 0.00581395 0.00426357 0.00426357 0.00454042 0.00531561
 0.00470653 0.00586932 0.00276855 0.00841639 0.00459579 0.00658915
 0.00459579 0.00404208 0.00437431 0.00415282 0.00537099 0.00271318
 0.00498339 0.01046512 0.00182724 0.00520487 0.00537099 0.00481728
 0.00503876 0.00753045 0.00669989 0.00697674 0.00636766 0.00681063
 0.00603544 0.00802879 0.00758583 0.00442968 0.00919158 0.00188261
 0.00669989 0.00609081 0.01677741 0.0072536  0.00681063 0.00808416
 0.00819491 0.00675526 0.0175526  0.00946844 0.00891473 0.01002215
 0.00874862 0.00786268 0.00913621 0.00869324 0.00924695 0.01024363
 0.00963455 0.00968992 0.01129568 0.01550388 0.0089701  0.01196013
 0.01052049 0.00963455 0.01445183 0.01184939 0.01472868 0.01057586
 0.01550388 0.01262458 0.01472868 0.01716501 0.01677741 0.01788483
 0.01688815 0.03228128 0.02978959 0.03947951 0.03023256 0.02834994
 0.03028793 0.01472868 0.03787375 0.04568106
]
\end{verbatim}

\begin{verbatim}
thetaNeg = [
 0.01089311 0.00082813 0.02433431 0.00127405 0.0052873  0.00598802
 0.00216588 0.0052236  0.00299401 0.0026755  0.00407695 0.00356733
 0.00350363 0.00401325 0.00382214 0.00286661 0.00293031 0.0026755
 0.00401325 0.0026118  0.00369474 0.00445917 0.00388585 0.00235699
 0.00356733 0.00687986 0.00847242 0.00433176 0.00426806 0.0025481
 0.00350363 0.00617913 0.00484138 0.00445917 0.00401325 0.00668875
 0.00369474 0.00910944 0.00930055 0.00700726 0.00439546 0.005351
 0.00503249 0.00490508 0.00439546 0.00496879 0.00356733 0.0054784
 0.00503249 0.00439546 0.0080265  0.00751688 0.00426806 0.01146643
 0.00624283 0.00605173 0.02089438 0.00713467 0.00847242 0.00598802
 0.0078991  0.00949165 0.01509746 0.00656135 0.00707096 0.00515989
 0.00687986 0.00910944 0.00719837 0.0081539  0.00981017 0.0107657
 0.00719837 0.01025608 0.00675245 0.01637151 0.01203975 0.01497006
 0.01000127 0.00866352 0.00834501 0.01184864 0.02350618 0.0186648
 0.0186011  0.01471525 0.01624411 0.01293158 0.01790037 0.01828258
 0.02350618 0.02229583 0.01637151 0.04459167 0.02057587 0.02420691
 0.03318894 0.04420945 0.03694738 0.05102561
]
\end{verbatim}

1.4 Multinomial Naive Bayes Classifier (MNBC) Testing+Evaluate Step

\medskip

For the first choice, MNBC classification accuracy is $0.765$. For the second choice, the accuracy is $0.815$. The sklearn package achieves the exact same results, $0.765$ and $0.815$.

\medskip

1.5 Multivariate Bernoulli Naive Bayes Classifier (BNBC)

\medskip

The resulting values of thetaPosTrue and thetaNegTrue are show below.

\begin{verbatim}
thetaPosTrue = [
 0.05128205 0.06552707 0.40883191 0.04131054 0.02279202 0.11680912
 0.10683761 0.14814815 0.0982906  0.0982906  0.08404558 0.0968661
 0.08119658 0.08974359 0.08547009 0.11253561 0.08974359 0.1011396
 0.07834758 0.12108262 0.0954416  0.0982906  0.0968661  0.12535613
 0.0968661  0.11823362 0.06125356 0.13817664 0.0997151  0.12393162
 0.1011396  0.06267806 0.0968661  0.1011396  0.11538462 0.05840456
 0.11396011 0.1965812  0.03846154 0.09259259 0.10541311 0.07834758
 0.11253561 0.14672365 0.13960114 0.14245014 0.13390313 0.14672365
 0.11396011 0.14672365 0.13247863 0.1025641  0.17236467 0.04558405
 0.13960114 0.12535613 0.31766382 0.14529915 0.13532764 0.17236467
 0.16096866 0.14102564 0.32193732 0.15669516 0.17663818 0.1951567
 0.17094017 0.16809117 0.18376068 0.18376068 0.18376068 0.21367521
 0.19230769 0.2037037  0.21509972 0.31054131 0.18233618 0.23646724
 0.17948718 0.15099715 0.20940171 0.20512821 0.3048433  0.21937322
 0.24216524 0.21652422 0.26638177 0.28205128 0.3048433  0.30769231
 0.2977208  0.49287749 0.41452991 0.56267806 0.45726496 0.41452991
 0.45584046 0.26210826 0.54273504 0.56552707
]
\end{verbatim}

\begin{verbatim}
thetaNegTrue = [
 0.18518519 0.01851852 0.34045584 0.02706553 0.10541311 0.11396011
 0.04558405 0.0997151  0.05982906 0.04700855 0.08262108 0.07264957
 0.06837607 0.07977208 0.07834758 0.06267806 0.05698006 0.04985755
 0.07692308 0.05698006 0.07549858 0.08547009 0.08119658 0.05128205
 0.07549858 0.13105413 0.15527066 0.08404558 0.08262108 0.05270655
 0.06837607 0.05555556 0.09259259 0.08974359 0.07407407 0.12108262
 0.07834758 0.16239316 0.15954416 0.08262108 0.07264957 0.08119658
 0.10541311 0.0954416  0.08547009 0.1011396  0.07264957 0.10826211
 0.08831909 0.08974359 0.11680912 0.14529915 0.06980057 0.19230769
 0.11253561 0.12250712 0.33903134 0.12393162 0.14814815 0.12393162
 0.15099715 0.16809117 0.25641026 0.11253561 0.12393162 0.0982906
 0.11396011 0.17663818 0.13817664 0.14529915 0.18518519 0.19373219
 0.14102564 0.17236467 0.12393162 0.28062678 0.20512821 0.24786325
 0.15669516 0.13390313 0.12535613 0.16524217 0.36324786 0.32193732
 0.25356125 0.2037037  0.26068376 0.20655271 0.29059829 0.28917379
 0.31908832 0.35042735 0.26638177 0.54273504 0.31766382 0.34472934
 0.45868946 0.48860399 0.49430199 0.57692308
]
\end{verbatim}

For the first choice, MNBC classification accuracy is $0.733$. For the second choice, the accuracy is $0.795$.

\medskip

\item
Sample QA Questions

\medskip

2.1 Bayes Classifier

\medskip

(a)
From the table

$$
P(G=1) = \frac{1}{2} ,\;
P(a=1|G=1) = \frac{1}{2} ,\;
P(b=1|G=1) = \frac{1}{4}
$$

$$
P(G=0) = \frac{1}{2} ,\;
P(a=1|G=0) = \frac{1}{2} ,\;
P(b=1|G=0) = \frac{1}{2}
$$

So

$$
P(a=1 \land b=1 \land G=1) = P(a=1|G=1)P(b=1|G=1)P(G=1) = \frac{1}{16}
$$

$$
P(a=1 \land b=1 \land G=0) = P(a=1|G=1)P(b=1|G=0)P(G=0) = \frac{1}{8}
$$

$$
P(a=1 \land b=1) = P(a=1 \land b=1 \land G=1) + P(a=1 \land b=1 \land G=0) = \frac{3}{16}
$$

$$
P(G=1|a=1 \land b=1) = \frac{P(a=1 \land b=1 \land G=1)}{P(a=1 \land b=1)} = \frac{1}{3}
$$

\medskip

(b)

False, NB is a generative model which gets $p(C|X)$ indirectly through modeling $p(C)$ and $p(X|C)$ while logistic regression is a discriminative model which indeed models $p(C|X)$

\medskip

(c)

False, they assume each feature $p(X_j|cluster==i)$ follows Gaussian distribution not the whole data point $X$.

\medskip

(d)

False, because Gaussian Naive Bayes classifier is a special case of QDA that the covariances are all diagonal matrices, but as long as the covariances are different, the decision boundary is quadratic as QDA.

% ========== Continue adding items as needed

\end{enumerate}

\end{document}
\grid
\grid

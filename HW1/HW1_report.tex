\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}

\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\begin{document}

% ========== Edit your name here
\author{Aobo Yang (ay6gv)}
\title{CS6316: HW1}
\maketitle

\medskip

% ========== Begin answering questions here
\begin{enumerate}

\item
Linear Algebra Review

1.1

$$ x_1=-1,\, x_2=0,\, x3=1 $$

1.2

$$
A = \begin{bmatrix}
    2 & 2 & 3 \\
    1 & -1 & 0 \\
    -1 & 2 & 1
\end{bmatrix},
\:
b = \begin{bmatrix}
    1 \\
    -1 \\
    2 \\
\end{bmatrix}
$$

1.3

$$
\begin{bmatrix}
    2 & 2 & 3 \\
    1 & -1 & 0 \\
    -1 & 2 & 1
\end{bmatrix}\begin{bmatrix}
    c_1 \\
    c_2 \\
    c_3 \\
\end{bmatrix} = \begin{bmatrix}
    0 \\
    0 \\
    0 \\
\end{bmatrix}
$$

Since $(c_1, c_2, c_3) = (0, 0, 0)$, the columns are linearly independent

The rank of $A$ is $3$

1.4

$$
A^{-1} = \begin{bmatrix}
    1 & -4 & -3 \\
    1 & -5 & -3 \\
    -1 & 6 & 4
\end{bmatrix}
$$

$$
det(A)= (2 \times -1 \times 1) + (3 \times 1 \times 2) - (3 \times -1 \times -1) - (2 \times 1 \times 1) = -1
$$

1.5
$$x = A^{-1}b = \begin{bmatrix}
    1 & -4 & -3 \\
    1 & -5 & -3 \\
    -1 & 6 & 4
\end{bmatrix}\begin{bmatrix}
    1 \\
    -1 \\
    2 \\
\end{bmatrix}=\begin{bmatrix}
    -1 \\
    0 \\
    1 \\
\end{bmatrix}$$

1.6

$$
\langle x,\, b \rangle = -1 \times 1 + 0 \times -1 + 1 \times 2 = 1,
$$

$$
x \otimes b = \begin{bmatrix}
    -1 \times 1 & -1 \times -1 & -1 \times 2 \\
    0 \times 1 & 0 \times -1 & 0 \times 2 \\
    1 \times 1 & 1 \times -1 & 1 \times 2
\end{bmatrix}=\begin{bmatrix}
    -1 & 1 & -2 \\
    0 & 0 & 0 \\
    1 & -1 & 2
\end{bmatrix}
$$

1.7

$$ L_1= |1| + |-1| + |2| = 4 $$
$$ L_2= (1^2 + (-1)^2 + 2^2)^\frac{1}{2} = \sqrt{6} $$
$$ L_\infty = 2 $$

1.8

$$
A_1 = \begin{bmatrix}
    2 & 2 & 3 \\
    1 & -1 & 0 \\
    -1 & 2 & 1 \\
    -1 & 2 & 1
\end{bmatrix},
\:
b = \begin{bmatrix}
    1 \\
    -1 \\
    2 \\
    1
\end{bmatrix}
$$

1.9

The rank is $A_1$ is also $3$

1.10

No, this cannot be solved. Because the last two equations has conflicts. $-x_1+2x_2+x_3$ cannot both be $2$ and $1$.

1.11

$$
A_1^+ = \begin{bmatrix}
    1 & -4 & -1.5 & -1.5 \\
    1 & -5 & -1.5 & -1.5 \\
    -1 & 6 & 2 & 2
\end{bmatrix}
$$

1.12

Yes, $B$ is orthogonal matrix because $B^TB=I$

1.13

$$ \frac{\partial (y^TAy)}{\partial y} = y^T(A + A^T)$$


\item
Linear Regression Model Fitting

2.2

The learned theta of the Normal Equation is $[3.0077,\, 1.6953]$, whose line is shown in Figure \ref{fig:ne}.

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
      \includegraphics[width=\linewidth]{figures/normal.png}
    \end{subfigure}
    \caption{Normal Equation}
    \label{fig:ne}
\end{figure}

The learned Theta of the Gradient Descent is $[3.0202,\, 1.6715]$, whose line and loss are shown in Figure \ref{fig:gd}.

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
      \includegraphics[width=\linewidth]{figures/gd.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\linewidth}
      \includegraphics[width=\linewidth]{figures/gd_loss.png}
    \end{subfigure}
    \caption{Gradient Descent}
    \label{fig:gd}
\end{figure}

The learned Theta of the Stochiastic Gradient Descent is $[3.0197,\, 1.6720]$, whose line and loss are shown in Figure \ref{fig:sgd}.


\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
      \includegraphics[width=\linewidth]{figures/sgd.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\linewidth}
      \includegraphics[width=\linewidth]{figures/sgd_loss.png}
    \end{subfigure}
    \caption{Stochiastic Gradient Descent}
    \label{fig:sgd}
\end{figure}

The learned Theta of the Minibatch Gradient Descent is $[3.0204,\, 1.6716]$, whose line and loss are shown in Figure \ref{fig:bsgd}.


\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
      \includegraphics[width=\linewidth]{figures/bsgd.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\linewidth}
      \includegraphics[width=\linewidth]{figures/bsgd_loss.png}
    \end{subfigure}
    \caption{Minibatch Gradient Descent}
    \label{fig:bsgd}
\end{figure}

% ========== Continue adding items as needed

\end{enumerate}

\end{document}
\grid
\grid

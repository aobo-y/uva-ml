{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMTFL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aobo-y/uva-ml/blob/master/AMTFL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZuX3ntQV85g",
        "colab_type": "text"
      },
      "source": [
        "# Deep Asymmetric Multi-task Feature Learning （AMTFL）\n",
        "\n",
        "Reproduced By: Aobo Yang, Yujia Mu, David Yao, Qi Liu\n",
        "\n",
        "Part of the following codes are from https://github.com/haebeom-lee/amtfl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZURTBlgLRbI",
        "colab_type": "text"
      },
      "source": [
        "**Import modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PqJUlHWLaHd",
        "colab_type": "code",
        "outputId": "7974f436-28f3-4c18-b43b-d56690487e4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-2MlOrsTBzu",
        "colab_type": "text"
      },
      "source": [
        "**Utilities**\n",
        "\n",
        "Create utilities to help logging & storing training results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae-cegFRRYo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log(*args):\n",
        "  time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "  print(f'{time}   ', *args)\n",
        "\n",
        "# shuffle any number of lists in same random order\n",
        "def shuffle(target, *args):\n",
        "  perm = np.random.permutation(len(target))\n",
        "  target = target[perm]\n",
        "\n",
        "  if args:\n",
        "    args = [arg[perm] for arg in args]\n",
        "    return [target, *args]\n",
        "  \n",
        "  return target\n",
        "\n",
        "class Accumulator():\n",
        "  def __init__(self, *args):\n",
        "    self.args = args\n",
        "    self.argdict = {}\n",
        "    for i, arg in enumerate(args):\n",
        "      self.argdict[arg] = i\n",
        "\n",
        "    self.results = [[] for _ in args]\n",
        "\n",
        "  def accum(self, val):\n",
        "    val = [val] if type(val) is not list else val\n",
        "    val = [v for v in val if v is not None]\n",
        "    assert(len(val) == len(self.args))\n",
        "    \n",
        "    for v, res in zip(val, self.results):\n",
        "      res.append(v)\n",
        "      \n",
        "  def clear(self):\n",
        "    self.results = [[] for _ in self.args]\n",
        "\n",
        "  # reduce: method of reducing values ['avg', 'sum']\n",
        "  def get(self, arg, reduce='avg'):\n",
        "    if arg in self.argdict:\n",
        "      idx = self.argdict[arg]\n",
        "      res = self.results[idx]\n",
        "      return np.mean(res) if reduce == 'avg' else np.sum(res)\n",
        "    \n",
        "    return None\n",
        "\n",
        "  def __str__(self):\n",
        "    return ', '.join(f'{arg}: {self.get(arg)}' for arg in self.args)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DFF0CShLpnd",
        "colab_type": "text"
      },
      "source": [
        "**Data preprocessing**\n",
        "\n",
        "\n",
        "Create an MNIST dataset whose tasks are imbalanced. Each number is treated as an independent task. As this work is about asymetric learning among tasks with different reliabilities, it requires to manually craft relatively unreliable tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M9zjqWl0dbx",
        "colab_type": "code",
        "outputId": "0d4a4999-e794-40aa-9982-a70d97e1778f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "def grp_mnist(x_data, y_data):\n",
        "  # group data by label\n",
        "  x_grp = [x_data[y_data==i] for i in range(10)]\n",
        "  y_grp = [y_data[y_data==i] for i in range(10)]\n",
        "\n",
        "  return x_grp, y_grp\n",
        "\n",
        "def onehoc_mnist_label(y_labels):\n",
        "  y_data = [[0] * 10 for _ in range(y_labels.shape[0])]\n",
        "  for y_record, idx in zip(y_data, y_labels):\n",
        "    y_record[idx] = 1\n",
        "\n",
        "  return np.array(y_data)\n",
        "\n",
        "def load_mnist_imb(path, grp_sizes=None):\n",
        "  mnist_trn, mnist_tst = mnist.load_data(path)   \n",
        "\n",
        "  x_trn_val, y_trn_val = mnist_trn\n",
        "  x_tst, y_tst = mnist_tst\n",
        "  \n",
        "  # print the size of original data\n",
        "  print('Original training data size:', x_trn_val.shape[0])\n",
        "  print('Original testing data size:', x_tst.shape[0])  \n",
        "  \n",
        "  # shuffle & group data\n",
        "  # x_trn_val, y_trn_val = shuffle(x_trn_val, y_trn_val)\n",
        "  x_grp, y_grp = grp_mnist(x_trn_val, y_trn_val)\n",
        "\n",
        "  # create imbalanced number of training data for each class, and the rest for validation\n",
        "  if not grp_sizes:\n",
        "    grp_sizes = [20 * (10 - i) for i in range(10)]\n",
        "\n",
        "  # split each grp to training & validation\n",
        "  # training\n",
        "  x_trn_grp = [grp[:size] for grp, size in zip(x_grp, grp_sizes)]\n",
        "  y_trn_grp = [grp[:size] for grp, size in zip(y_grp, grp_sizes)]\n",
        "\n",
        "  # validation\n",
        "  x_val_grp = [grp[size:] for grp, size in zip(x_grp, grp_sizes)]\n",
        "  y_val_grp = [grp[size:] for grp, size in zip(y_grp, grp_sizes)]\n",
        "  \n",
        "  # print data per task\n",
        "  for i in range(10):\n",
        "    print(f'Task {i}: trn size {x_trn_grp[i].shape[0]}, val size {x_val_grp[i].shape[0]}')\n",
        "  \n",
        "  # concat the training data and validation groups\n",
        "  x_trn, y_trn = np.concatenate(x_trn_grp, axis=0), np.concatenate(y_trn_grp, axis=0)\n",
        "  x_val, y_val = np.concatenate(x_val_grp, axis=0), np.concatenate(y_val_grp, axis=0)\n",
        "  \n",
        "  # testing\n",
        "  x_grp, y_grp = grp_mnist(x_tst, y_tst)\n",
        "  x_tst_grp = [grp[:1000] for grp in x_grp]\n",
        "  y_tst_grp = [grp[:1000] for grp in y_grp]\n",
        "  x_tst, y_tst = np.concatenate(x_tst_grp, axis=0), np.concatenate(y_tst_grp, axis=0)\n",
        "\n",
        "  # normalize color into 0-1\n",
        "  x_trn, x_val, x_tst = (x / 255.0 for x in (x_trn, x_val, x_tst))\n",
        "  # onehoc mapping for label\n",
        "  y_trn, y_val, y_tst = (onehoc_mnist_label(y) for y in (y_trn, y_val, y_tst))\n",
        "\n",
        "  # print splitted data\n",
        "  print('training data size:', x_trn.shape[0])\n",
        "  print('validation data size:', x_val.shape[0])\n",
        "  print('testing data size:', x_tst.shape[0]) \n",
        "\n",
        "  return x_trn, y_trn, x_val, y_val, x_tst, y_tst\n",
        "\n",
        "\n",
        "# number list to specify the number of training records per task, use 200, 180, . . . , 20 samples respectively\n",
        "grp_sizes = [20 * (10 - i) for i in range(10)]\n",
        "mnist_imb = load_mnist_imb('./mnist', grp_sizes=grp_sizes)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original training data size: 60000\n",
            "Original testing data size: 10000\n",
            "Task 0: trn size 200, val size 5723\n",
            "Task 1: trn size 180, val size 6562\n",
            "Task 2: trn size 160, val size 5798\n",
            "Task 3: trn size 140, val size 5991\n",
            "Task 4: trn size 120, val size 5722\n",
            "Task 5: trn size 100, val size 5321\n",
            "Task 6: trn size 80, val size 5838\n",
            "Task 7: trn size 60, val size 6205\n",
            "Task 8: trn size 40, val size 5811\n",
            "Task 9: trn size 20, val size 5929\n",
            "training data size: 1100\n",
            "validation data size: 58900\n",
            "testing data size: 9786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVeNU2_CuR7X",
        "colab_type": "text"
      },
      "source": [
        "**Model**\n",
        "\n",
        "Create a basic Lenet-Conv model with Tensorflow. It returns a dictionary of parameters which may be used in downstream training objectives. Within it, *h*, the hidden representation is required by AMTFL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrZplmQ-RYzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "relu = tf.nn.relu\n",
        "Flatten = tf.keras.layers.Flatten\n",
        "Conv2D = tf.keras.layers.Conv2D\n",
        "MaxPooling2D = tf.keras.layers.MaxPooling2D\n",
        "Dense = tf.keras.layers.Dense\n",
        "\n",
        "# ensure the log variable is a probability larger than 0\n",
        "safe_log = lambda x: tf.log(tf.clip_by_value(x, 1e-10, 1.0))\n",
        "\n",
        "def accuracy(logits, labels):\n",
        "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "    return tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    \n",
        "def create_lenet(x, y, name='lenet'):\n",
        "    x = tf.expand_dims(x, -1)\n",
        "    x = Conv2D(20, 5, name=name+'/conv1')(x)\n",
        "    x = relu(x)\n",
        "    x = MaxPooling2D(name=name+'/pool1')(x)\n",
        "    x = Conv2D(50, 5, name=name+'/conv2')(x)\n",
        "    x = relu(x)\n",
        "    x = MaxPooling2D(name=name+'/pool2')(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # 2nd last hidden representation\n",
        "    h = Dense(500, activation=relu, name=name+'/dense1')(x)\n",
        "\n",
        "    # last later, task specific weights\n",
        "    with tf.variable_scope(name + '/fc_last', reuse=tf.AUTO_REUSE):\n",
        "      S = tf.get_variable('SW', [500, 10], initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "      Sb = tf.get_variable('Sb', [10], initializer=tf.zeros_initializer())\n",
        "      preds = tf.matmul(h, S) + Sb\n",
        "\n",
        "    all_vars = tf.get_collection('variables', scope=name)\n",
        "\n",
        "    return {\n",
        "      'name': name,\n",
        "      'preds': preds,\n",
        "      'h': h,\n",
        "      'S': S,\n",
        "      'L': [v for v in all_vars if 'fc_last' not in v.name],\n",
        "      'weights': [v for v in all_vars],\n",
        "      'acc': accuracy(preds, y)\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIBbluwiYy6b",
        "colab_type": "text"
      },
      "source": [
        "**Objective Definition**\n",
        "\n",
        "Define functions to wrap given arbitry Multi-Task neural network model with CrossEntropy loss and AMTFL respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4awGvN-ZdaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy(logits, y):\n",
        "  y_hat = tf.sigmoid(logits)\n",
        "  cent = -y * safe_log(y_hat) - (1 - y) * safe_log(1 - y_hat)\n",
        "  return cent\n",
        "\n",
        "# ordinary cross-entropy loss\n",
        "def normal_ce(model, mu=1e-4, lambda_=1e-4):\n",
        "  preds, S, L, acc = (model[k] for k in ['preds', 'S', 'L', 'acc'])\n",
        "  task_loss = tf.reduce_sum(tf.reduce_mean(cross_entropy(preds, y), 0))\n",
        "\n",
        "  l1_S = mu * tf.reduce_sum(tf.abs(S))\n",
        "  l2_L = lambda_ * tf.add_n([tf.nn.l2_loss(var) for var in L])\n",
        "\n",
        "  loss = task_loss + l1_S + l2_L\n",
        "\n",
        "  return {\n",
        "    'model': model,\n",
        "    'task_loss': task_loss,\n",
        "    'loss': loss,\n",
        "    'acc': acc\n",
        "  }\n",
        "\n",
        "# AMTFL cross-entropy loss\n",
        "def amtfl_ce(model, alpha, gamma, mu=1e-4, lambda_=1e-4):\n",
        "  preds, h, S, L, acc, name = (model[k] for k in ['preds', 'h', 'S', 'L', 'acc', 'name'])\n",
        "\n",
        "  task_losses = tf.reduce_mean(cross_entropy(preds, y), 0)\n",
        "\n",
        "  # reconstruct hidden representation with prediction\n",
        "  with tf.variable_scope(name + '/recon', reuse=tf.AUTO_REUSE):\n",
        "    AW = tf.get_variable('AW', [10, 500], initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "    Ab = tf.get_variable('Ab', [500], initializer=tf.zeros_initializer())\n",
        "    h_hat = relu(tf.matmul(preds, AW) + Ab)\n",
        "\n",
        "  # sum over all features & mean for all records\n",
        "  # recon_loss = gamma * tf.reduce_mean(tf.reduce_sum((h - h_hat) ** 2, 1))\n",
        "  recon_loss = gamma * tf.nn.l2_loss(h - h_hat)\n",
        "\n",
        "  a = 1 + alpha * tf.reduce_sum(tf.abs(AW), 1)\n",
        "  cent_loss = tf.reduce_sum(a * task_losses)\n",
        "\n",
        "  # keep track of task only losses to understand the performance\n",
        "  task_loss = tf.reduce_sum(task_losses, 0)\n",
        "  \n",
        "  l1_S = mu * tf.reduce_sum(tf.abs(S))\n",
        "  l2_L = lambda_ * tf.add_n([tf.nn.l2_loss(var) for var in L])\n",
        "\n",
        "  loss = cent_loss + recon_loss + l1_S + l2_L\n",
        "\n",
        "  return {\n",
        "    'model': model,\n",
        "    'task_loss': task_loss,\n",
        "    'loss': loss,\n",
        "    'A': AW,\n",
        "    'S': S,\n",
        "    'acc': acc\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR-FiEb5XORh",
        "colab_type": "text"
      },
      "source": [
        "**Model Creation**\n",
        "\n",
        "Create two exactly same Lenet-Conv models. Wrap one with ordianry CrossEntropy loss and the other with AMTFL regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pYkMDLGSTbP",
        "colab_type": "code",
        "outputId": "abda7efd-af8c-4938-880f-59cc8562e27a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "alpha =  0.2 \n",
        "gamma =  0.004\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, 28, 28])\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "lenet_ce = create_lenet(x, y, name='lenet_ce')  # MT-CNN\n",
        "lenet_amtfl = create_lenet(x, y, name='lenet_amtfl')\n",
        "\n",
        "ce_params = normal_ce(lenet_amtfl)\n",
        "amtfl_params = amtfl_ce(lenet_amtfl, alpha, gamma)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEhdjduRjJ7u",
        "colab_type": "text"
      },
      "source": [
        "**Define training & testing functions**\n",
        "\n",
        "Define general training and accuracy testing functions, which work with both models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf4zwzIjSe6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train model\n",
        "def train(model, x_trn, y_trn, x_val, y_val, batch_size=100, n_epochs=1000, print_every=100):\n",
        "  assert len(x_trn) == len(y_trn)\n",
        "  assert len(x_val) == len(y_val)\n",
        "\n",
        "  # create Adam optimizer\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "  lr = tf.train.piecewise_constant(tf.cast(global_step, tf.float32), [len(x_trn) * n_epochs / 2 / batch_size], [1e-4, 1e-5])\n",
        "  opt = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "  # training objective\n",
        "  loss = model['loss']\n",
        "  trn_obj = opt.minimize(loss, global_step=global_step)\n",
        "\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  trn_args = ['loss', 'task_loss', 'acc']\n",
        "  trn_to_run = [trn_obj, *[model[arg] for arg in trn_args]]\n",
        "  \n",
        "  train_accum = Accumulator(*trn_args)\n",
        "\n",
        "  val_args = ['loss', 'task_loss', 'acc']\n",
        "  val_accum = Accumulator(*val_args)\n",
        "  val_to_run = [model[arg] for arg in val_args]\n",
        "  \n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    if epoch % print_every == 0:\n",
        "      log(f'Epoch {epoch} start...')\n",
        "\n",
        "    # shuffle training data\n",
        "    x_trn, y_trn = shuffle(x_trn, y_trn)\n",
        "\n",
        "    train_accum.clear()\n",
        "\n",
        "    # Train\n",
        "    for i in range(0, len(x_trn), batch_size):\n",
        "      x_batch = x_trn[i:i+batch_size]\n",
        "      y_batch = y_trn[i:i+batch_size]\n",
        "      train_result = sess.run(trn_to_run, {x: x_batch, y: y_batch})\n",
        "      train_accum.accum(train_result)\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "      log('training results -', train_accum)\n",
        "\n",
        "      # Validation\n",
        "      val_accum.clear()\n",
        "      for j in range(0, len(x_val), batch_size):\n",
        "        x_batch = x_val[j:j+batch_size]\n",
        "        y_batch = y_val[j:j+batch_size]\n",
        "        val_result = sess.run(val_to_run, {x: x_batch, y: y_batch})\n",
        "        val_accum.accum(val_result)\n",
        "\n",
        "      log('validation results -', val_accum)\n",
        "\n",
        "      print()\n",
        "  \n",
        "  return sess    \n",
        "\n",
        "\n",
        "# Test accuracy\n",
        "def test_acc(tst_model, sess, x_tst, y_tst):\n",
        "  assert len(x_tst) == len(y_tst)\n",
        "\n",
        "  return sess.run(tst_model['acc'], {x: x_tst, y: y_tst})\n",
        "\n",
        "# Test per class accuracy\n",
        "def test_acc_per_class(tst_model, sess, x_tst, y_tst):\n",
        "  assert len(x_tst) == len(y_tst)\n",
        "  preds = sess.run(tst_model['model']['preds'], {x: x_tst, y: y_tst})\n",
        "  preds = np.argmax(preds, 1)\n",
        "\n",
        "  matches = [0] * 10\n",
        "  counts = [0] * 10\n",
        "  for label, pred in zip(np.argmax(y_tst, 1), preds):\n",
        "    if label == pred:\n",
        "      matches[label] += 1\n",
        "    counts[label] += 1\n",
        "\n",
        "  return np.array(matches) / np.array(counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l8jKM5iSHID",
        "colab_type": "text"
      },
      "source": [
        "**Training**\n",
        "\n",
        "Train two Lenets. One uses ordinary CrossEntropy with L1 and L2regularization. The other one applies the AMTFL in addition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aodhAOS3bf4h",
        "colab_type": "code",
        "outputId": "19ba7f18-985a-4b72-c785-3d085b5e8a56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 100\n",
        "n_epochs = 1000\n",
        "\n",
        "x_trn, y_trn, x_val, y_val, x_tst, y_tst = mnist_imb\n",
        "\n",
        "# train normal\n",
        "print('Train model with normal loss')\n",
        "sess_ce = train(ce_params, x_trn, y_trn, x_val, y_val, batch_size, n_epochs)\n",
        "\n",
        "print()\n",
        "\n",
        "# train AMTFL\n",
        "print('Train model with AMTFL')\n",
        "sess_amtfl = train(amtfl_params, x_trn, y_trn, x_val, y_val, batch_size, n_epochs)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train model with normal loss\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2019-12-07 22:46:55    Epoch 100 start...\n",
            "2019-12-07 22:46:55    training results - loss: 0.32507747411727905, task_loss: 0.28764376044273376, acc: 0.9627271890640259\n",
            "2019-12-07 22:46:57    validation results - loss: 0.8588686585426331, task_loss: 0.821418821811676, acc: 0.8693039417266846\n",
            "\n",
            "2019-12-07 22:47:02    Epoch 200 start...\n",
            "2019-12-07 22:47:02    training results - loss: 0.08196146041154861, task_loss: 0.04174843430519104, acc: 1.0\n",
            "2019-12-07 22:47:03    validation results - loss: 0.9287732243537903, task_loss: 0.8885499835014343, acc: 0.8981664180755615\n",
            "\n",
            "2019-12-07 22:47:09    Epoch 300 start...\n",
            "2019-12-07 22:47:09    training results - loss: 0.04803488031029701, task_loss: 0.007232029922306538, acc: 1.0\n",
            "2019-12-07 22:47:10    validation results - loss: 1.1523287296295166, task_loss: 1.1115282773971558, acc: 0.9009338021278381\n",
            "\n",
            "2019-12-07 22:47:16    Epoch 400 start...\n",
            "2019-12-07 22:47:16    training results - loss: 0.042162563651800156, task_loss: 0.002562939887866378, acc: 1.0\n",
            "2019-12-07 22:47:17    validation results - loss: 1.2551939487457275, task_loss: 1.2156051397323608, acc: 0.9027503728866577\n",
            "\n",
            "2019-12-07 22:47:22    Epoch 500 start...\n",
            "2019-12-07 22:47:22    training results - loss: 0.038290880620479584, task_loss: 0.001416873070411384, acc: 1.0\n",
            "2019-12-07 22:47:24    validation results - loss: 1.297767162322998, task_loss: 1.2609126567840576, acc: 0.9035483002662659\n",
            "\n",
            "2019-12-07 22:47:29    Epoch 600 start...\n",
            "2019-12-07 22:47:29    training results - loss: 0.037687722593545914, task_loss: 0.0012984119821339846, acc: 1.0\n",
            "2019-12-07 22:47:31    validation results - loss: 1.3002309799194336, task_loss: 1.2638449668884277, acc: 0.9037520885467529\n",
            "\n",
            "2019-12-07 22:47:36    Epoch 700 start...\n",
            "2019-12-07 22:47:36    training results - loss: 0.03685670346021652, task_loss: 0.0012190029956400394, acc: 1.0\n",
            "2019-12-07 22:47:37    validation results - loss: 1.2951942682266235, task_loss: 1.2595616579055786, acc: 0.9042954444885254\n",
            "\n",
            "2019-12-07 22:47:42    Epoch 800 start...\n",
            "2019-12-07 22:47:43    training results - loss: 0.03561895713210106, task_loss: 0.00111951504368335, acc: 1.0\n",
            "2019-12-07 22:47:44    validation results - loss: 1.2945239543914795, task_loss: 1.2600317001342773, acc: 0.9046688675880432\n",
            "\n",
            "2019-12-07 22:47:49    Epoch 900 start...\n",
            "2019-12-07 22:47:49    training results - loss: 0.033966075628995895, task_loss: 0.0010211411863565445, acc: 1.0\n",
            "2019-12-07 22:47:51    validation results - loss: 1.2920960187911987, task_loss: 1.2591606378555298, acc: 0.9048725962638855\n",
            "\n",
            "2019-12-07 22:47:56    Epoch 1000 start...\n",
            "2019-12-07 22:47:56    training results - loss: 0.032003406435251236, task_loss: 0.0009358737734146416, acc: 1.0\n",
            "2019-12-07 22:47:57    validation results - loss: 1.2859619855880737, task_loss: 1.254905343055725, acc: 0.9051273465156555\n",
            "\n",
            "\n",
            "Train model with AMTFL\n",
            "2019-12-07 22:48:04    Epoch 100 start...\n",
            "2019-12-07 22:48:04    training results - loss: 2.802800178527832, task_loss: 2.3719539642333984, acc: 0.6709091067314148\n",
            "2019-12-07 22:48:05    validation results - loss: 3.5454392433166504, task_loss: 2.903907299041748, acc: 0.44981324672698975\n",
            "\n",
            "2019-12-07 22:48:11    Epoch 200 start...\n",
            "2019-12-07 22:48:11    training results - loss: 1.671291708946228, task_loss: 0.9968423247337341, acc: 0.9436363577842712\n",
            "2019-12-07 22:48:13    validation results - loss: 2.5930163860321045, task_loss: 1.7100876569747925, acc: 0.7527503967285156\n",
            "\n",
            "2019-12-07 22:48:18    Epoch 300 start...\n",
            "2019-12-07 22:48:18    training results - loss: 0.3184698820114136, task_loss: 0.1258220076560974, acc: 0.9990909099578857\n",
            "2019-12-07 22:48:20    validation results - loss: 1.5403884649276733, task_loss: 0.7843071818351746, acc: 0.8884719610214233\n",
            "\n",
            "2019-12-07 22:48:26    Epoch 400 start...\n",
            "2019-12-07 22:48:26    training results - loss: 0.12411019951105118, task_loss: 0.023148832842707634, acc: 1.0\n",
            "2019-12-07 22:48:27    validation results - loss: 1.3113406896591187, task_loss: 0.6096071600914001, acc: 0.9266554117202759\n",
            "\n",
            "2019-12-07 22:48:33    Epoch 500 start...\n",
            "2019-12-07 22:48:33    training results - loss: 0.08897548168897629, task_loss: 0.008176312781870365, acc: 1.0\n",
            "2019-12-07 22:48:35    validation results - loss: 1.2776198387145996, task_loss: 0.5880469679832458, acc: 0.9337520599365234\n",
            "\n",
            "2019-12-07 22:48:41    Epoch 600 start...\n",
            "2019-12-07 22:48:41    training results - loss: 0.08571218699216843, task_loss: 0.007291905116289854, acc: 1.0\n",
            "2019-12-07 22:48:43    validation results - loss: 1.2905057668685913, task_loss: 0.5956727862358093, acc: 0.9335823655128479\n",
            "\n",
            "2019-12-07 22:48:49    Epoch 700 start...\n",
            "2019-12-07 22:48:49    training results - loss: 0.08327866345643997, task_loss: 0.006475500296801329, acc: 1.0\n",
            "2019-12-07 22:48:50    validation results - loss: 1.2908604145050049, task_loss: 0.5964640974998474, acc: 0.9340407848358154\n",
            "\n",
            "2019-12-07 22:48:56    Epoch 800 start...\n",
            "2019-12-07 22:48:56    training results - loss: 0.08079223334789276, task_loss: 0.00570637546479702, acc: 1.0\n",
            "2019-12-07 22:48:57    validation results - loss: 1.2889329195022583, task_loss: 0.5964246392250061, acc: 0.934465229511261\n",
            "\n",
            "2019-12-07 22:49:03    Epoch 900 start...\n",
            "2019-12-07 22:49:03    training results - loss: 0.07830973714590073, task_loss: 0.00503131840378046, acc: 1.0\n",
            "2019-12-07 22:49:05    validation results - loss: 1.2862383127212524, task_loss: 0.5964298248291016, acc: 0.9348726868629456\n",
            "\n",
            "2019-12-07 22:49:11    Epoch 1000 start...\n",
            "2019-12-07 22:49:11    training results - loss: 0.07596058398485184, task_loss: 0.004461059346795082, acc: 1.0\n",
            "2019-12-07 22:49:12    validation results - loss: 1.2834856510162354, task_loss: 0.5963162779808044, acc: 0.9355177283287048\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bcZOdrGFstL",
        "colab_type": "text"
      },
      "source": [
        "**Test**\n",
        "\n",
        "Test and compare the accuracies. The results should show that the accuracy of AMTFL is higher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzN856ZhFrp1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "54da1e1f-00ff-419e-8023-9575fb488eb1"
      },
      "source": [
        "# test normal\n",
        "print('Normal model\\'s accuracy:', test_acc(ce_params, sess_ce, x_tst, y_tst))\n",
        "\n",
        "# test AMTFL\n",
        "print('AMTFL model\\'s accuracy:', test_acc(amtfl_params, sess_amtfl, x_tst, y_tst))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal model's accuracy: 0.90905374\n",
            "AMTFL model's accuracy: 0.93919885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vz84it4ayKI",
        "colab_type": "text"
      },
      "source": [
        "**Analysis**\n",
        "\n",
        "Generate the task transfer matrix ($AS$) to visualize how inter-task knowledge transfer is done in Deep-AMTFL. The figure illustrates the asymmetric transfer. Easier tasks should transfer more to harder tasks, but not vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VycBRxBiB71-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "ec922dc8-cf4e-47f1-c016-250a83c9d4e4"
      },
      "source": [
        "# print(test_acc_per_class(amtfl_params, sess_amtfl, x_tst, y_tst))\n",
        "trans_mat = tf.abs(tf.matmul(amtfl_params['A'], amtfl_params['S'])).eval(session=sess_amtfl)\n",
        "\n",
        "plt.imshow(trans_mat)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fde801645c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1FIx/wOXG7G/duwL2AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}